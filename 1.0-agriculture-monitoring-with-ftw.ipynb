{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13i7KQ9t-CV8"
   },
   "source": [
    "# Agricultural Monitoring with Fields of The World (FTW)\n",
    "\n",
    "This tutorial demonstrates how to generate field boundaries globally using the Fields of The World dataset, pretrained models, and command line interface (CLI). You will then use those boundaries in two example downstream agricultural monitoring tasks: crop type classification and forest loss monitoring.\n",
    "\n",
    "By the end, users will be able to perform the following tasks to support agriculture-related decision-making:\n",
    "1. Extract agricultural field boundaries for any location\n",
    "2. Build machine learning models for crop type classification\n",
    "3. Analyze forest loss within agricultural landscapes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCdqVD548pCs"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fieldsoftheworld/iclr2026-ml4rs-tutorial/blob/main/1.0-agriculture-monitoring-with-ftw.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNv0ANr5WcD_"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "\n",
    "*   [Overview](#overview)\n",
    "*   [Real-World Impact](#impact)\n",
    "*   [Target Audience](#target-audience)\n",
    "*   [Background & Prerequisites](#background-and-prereqs)\n",
    "*   [Software Requirements](#software-requirements)\n",
    "*   [Data Description](#data-description)\n",
    "*   [Methodology](#methodology)\n",
    "*   [Final Discussion & Takeaways](#final-discussion-takeaways)\n",
    "*   [References](#references)\n",
    "*   [Authors and Contact Information](#authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH81wjfsJsv1"
   },
   "source": [
    "<a name=\"overview\"></a>  \n",
    "# üåç Overview  \n",
    "\n",
    "This tutorial introduces participants to the [Fields of The World (FTW)](https://fieldsofthe.world/) ecosystem (benchmark dataset, pretrained segmentation models, and command-line interface) and demonstrates how it can be used to address pressing agricultural challenges under climate change. The tutorial‚Äôs main contribution is to show how users can generate field boundaries anywhere globally and then use these boundaries for downstream machine learning tasks in agricultural monitoring.  \n",
    "\n",
    "Specifically, users will:  \n",
    "1. üìê Learn to apply FTW and open-source geospatial libraries to extract field boundaries tailored to their region of interest.  \n",
    "2. üçö Use the field boundaries as inputs for crop type classification, highlighting how machine learning methods can support food security and climate resilience.  \n",
    "3. üå≥ Apply the boundaries for forest loss monitoring within agricultural landscapes, connecting land use change to climate impacts and policies such as the EU Deforestation Regulation (EUDR).  \n",
    "\n",
    "The goal of the tutorial is to give users hands-on experience ‚úãüñ•Ô∏è with an end-to-end pipeline that links data, tools, and methods for climate-related agricultural monitoring üîó. By the end, participants will be able to:  \n",
    "1. üó∫Ô∏è Generate and manipulate field boundaries,  \n",
    "2. ü§ñ Apply machine learning models for crop type classification, and  \n",
    "3. üå≥ Monitor deforestation impacts in agricultural fields.  \n",
    "\n",
    "These outcomes empower users to integrate geospatial datasets and machine learning approaches for climate change mitigation and adaptation in agriculture.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99jkSa_KmrDH"
   },
   "source": [
    "<a name=\"impact\"></a>\n",
    "# üåé Real-World Impact\n",
    "\n",
    "### Pathways to Positive Impact  \n",
    "Agriculture both drives and is affected by climate change. Agricultural expansion and poor land management contribute significantly to deforestation, biodiversity loss, and greenhouse gas emissions. On the other hand, climate change itself threatens crop yields, food security, and rural livelihoods. Accurate, up-to-date data about where fields are located and how they are being managed is essential for building sustainable and resilient food systems. However, this information is often unavailable, outdated, or fragmented in many parts of the world.  \n",
    "\n",
    "This tutorial introduces the **Fields of The World (FTW)** machine learning ecosystem and demonstrates how it can be used to automatically generate field boundaries globally. These boundaries provide a foundation for downstream monitoring tasks like crop type classification and forest loss detection, which are critical for climate-related decision-making.  \n",
    "\n",
    "#### Who will use these models and how?  \n",
    "* üåç **Policymakers and regulators** (e.g., under the EU Deforestation Regulation, EUDR) can use field-level monitoring to verify compliance with deforestation-free supply chains. This ensures that commodities like soy, palm oil, and cocoa are not driving illegal forest loss.  \n",
    "* üå± **NGOs, development organizations, and agencies** can use the models to track agricultural expansion and design interventions that protect vulnerable ecosystems while supporting farmers.  \n",
    "* üè≠ **Private sector actors** (e.g., food companies, climate-tech startups, or Monitoring Reporting and Verification (MRV) service providers) can integrate field boundary datasets and models into supply chain monitoring systems, helping corporations meet sustainability pledges and reduce climate risk.  \n",
    "* üë©‚Äçüåæ **Farmers and cooperatives** can benefit indirectly through improved access to climate finance, climate-smart certifications, carbon markets, or targeted advisory services built on these monitoring tools.  \n",
    "\n",
    "#### Real-world impact and decision pathways  \n",
    "1. üåæ **Crop type classification** models support yield forecasting, production estimation, and food security planning. Governments and international agencies can anticipate shortages and mobilize relief efforts earlier, reducing vulnerability to climate-driven shocks. In-season crop type maps enable rapid response assessments of damage to food production when disasters such as floods, fires, or tornadoes occur.  \n",
    "2. üå≥ **Forest loss monitoring** within agricultural landscapes helps identify where land-use change is contributing to emissions and biodiversity loss. This informs land-use policy, conservation programs, and corporate sustainability audits and helps farmers/producers provide evidence for sustainable supply chain certifications.  \n",
    "3. ‚è±Ô∏è Combining these tools allows for **near-real-time, transparent monitoring** of agriculture and forest interactions at the field scale.  \n",
    "\n",
    "#### System-level outcomes  \n",
    "By equipping users with the ability to generate field boundaries and link them to climate-relevant monitoring tasks, this tutorial lowers the barrier for researchers, practitioners, and policymakers to access and deploy advanced geospatial AI.\n",
    "\n",
    "The impact pathway extends from **technical outputs** (field boundaries, crop maps, forest loss detections) ‚Üí to **actionable insights** (crop yield forecasts, deforestation alerts) ‚Üí to **systemic changes** in agricultural management, supply chain governance, and climate adaptation strategies.\n",
    "\n",
    "These methods have the potential to reduce emissions, protect ecosystems, and build resilience for farming communities worldwide.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5sbM_JPpdMR"
   },
   "source": [
    "<a name=\"target-audience\"></a>\n",
    "# üéØ Target Audience\n",
    "\n",
    "This tutorial is designed for **researchers, practitioners, and students working at the intersection of agriculture, remote sensing, and machine learning**.\n",
    "\n",
    "Specifically, it is intended for:  \n",
    "\n",
    "* üå± **Climate and agricultural scientists** who may not have a strong background in machine learning but want to learn how modern geospatial AI tools can be applied to monitor crop production and forest loss under climate change.  \n",
    "* ü§ñ **Machine learning researchers and data scientists** with prior experience in deep learning or geospatial analysis who are seeking real-world, climate-relevant applications to test and adapt their methods.  \n",
    "* üèõÔ∏è **Policymakers, NGOs, and sustainability professionals** interested in understanding how field-scale monitoring tools can support compliance with sustainability standards (e.g., the EU Deforestation Regulation), improve food security planning, and inform conservation decisions.  \n",
    "* üéì **Graduate students and early-career researchers** who want hands-on experience with open datasets, tools, and workflows that bridge technical machine learning methods with applied climate and agricultural challenges.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQgijl46pYzn"
   },
   "source": [
    "<a name=\"background-and-prereqs\"></a>\n",
    "# üìì Background & Prerequisites\n",
    "\n",
    "To get the most out of this tutorial, participants should have:  \n",
    "* üêç **Basic Python programming skills** (variables, loops, functions, and libraries such as `numpy` and `pandas`).  \n",
    "* üìì Familiarity with **Jupyter notebooks** as an interactive environment for running code and exploring data.  \n",
    "* ü§ñ A general understanding of **machine learning** (e.g., training and evaluating models) and **geospatial data** (e.g., satellite imagery, vector vs. raster data) is helpful but not required.  \n",
    "\n",
    "No prior experience with agricultural monitoring or the Fields of The World dataset is required.  \n",
    "\n",
    "### Key Concepts  \n",
    "\n",
    "To successfully follow the tutorial, users should be comfortable with the following concepts:  \n",
    "\n",
    "* üó∫Ô∏è **Field boundaries**: Polygons that delineate agricultural fields on the Earth‚Äôs surface. They provide a spatial unit for monitoring and analysis, enabling crop type classification, yield prediction, and land-use change assessment.  \n",
    "* üõ∞Ô∏è **Remote sensing / Earth observations**: Data captured by satellites (e.g., Sentinel-2, Landsat) that provide multi-spectral views of Earth. Remote sensing allows large-scale, repeatable, and timely monitoring of agriculture and forests.  \n",
    "* üìê **Vector vs. raster data**: Vector data (points, lines, polygons) represent discrete features such as field boundaries, while raster data represent gridded continuous surfaces such as satellite images or deforestation maps.  \n",
    "* üåæ **Crop type classification**: A machine learning task that assigns labels (e.g., maize, soy, wheat) to agricultural fields based on spectral and temporal information from satellite imagery. This supports food security planning and yield forecasting.  \n",
    "* üå≥ **Forest loss monitoring**: The detection of deforestation events, often using global datasets (e.g., Hansen Global Forest Change). Linking these to field boundaries helps assess whether agricultural expansion is driving forest loss.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSRCNgYzUwaf"
   },
   "source": [
    "<a name=\"software-requirements\"></a>\n",
    "# üíª Software Requirements\n",
    "\n",
    "This notebook requires Python (>= 3.12 and < 3.14) and an environment to run Jupyter notebooks.\n",
    "\n",
    "All requirements can be installed through `pip` by running:\n",
    "```bash\n",
    "pip install .\n",
    "```\n",
    "\n",
    "If you are using Conda, we provide a lightweight environment file `environment.yml`. You can install it by running:\n",
    "```bash\n",
    "# Install environment\n",
    "conda env create -f environment.yml\n",
    "# Activate environment\n",
    "conda activate ftw-tutorial\n",
    "```\n",
    "\n",
    "<!--\n",
    "\n",
    "All tutorial submissions must have a separate `requirements.txt` file stating all dependencies and versions for software, packages, or tooling used in the tutorial. This file allows tutorial users who would like to run a copy of the notebook locally with all dependencies needed to create their own virtual environment/IPython kernel.\n",
    "\n",
    "In bash you may create a `requirements.txt` by typing the following command:\n",
    "\n",
    "`pip freeze > requirements.txt'\n",
    "\n",
    "Avoid undocumented dependencies.\n",
    "\n",
    "We encourage explicitly printing out your dependencies using a notebook extension such as [watermark](https://github.com/rasbt/watermark) at the bottom of your notebook.\n",
    "\n",
    "Within the notebook, there must be a cell which includes the necessary directory structure to retrieve files or data using relative paths for ease of use. If data is private, please provide a similar public sample for those that would like to reproduce or extend your work. Instructions and imports with versioning must be provided and clearly labeled at the top.\n",
    "\n",
    "Document the beginning and end date of your analysis to contextualize the development stage of the notebook in the event that highlighted packages or content is updated in the future.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xy2_sTYE-uGi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = (\n",
    "    \"TRUE\"  # when multiple copies of OpenMP are found we get a warning, this silences it\n",
    ")\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# If you have GDAL installed in your environment, these environment variables can cause issues\n",
    "# with rasterio and fiona. So we unset them here.\n",
    "for entry in [\"GDAL_DATA\", \"PROJ_NETWORK\", \"PROJ_DATA\", \"GDAL_DRIVER_PATH\"]:\n",
    "    if entry in os.environ:\n",
    "        del os.environ[entry]\n",
    "\n",
    "# If the notebook was opened in Colab then clone the source repo and install required packages\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    !git clone https://github.com/fieldsoftheworld/iclr2026-ml4rs-tutorial.git\n",
    "    os.chdir(\"iclr2026-ml4rs-tutorial\")\n",
    "\n",
    "    !pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkAA4X_V8pCt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import leafmap\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import torch\n",
    "from mgrs import MGRS\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXoiLncsU3pe"
   },
   "source": [
    "<a name=\"data-description\"></a>\n",
    "# üóÇÔ∏è Data Description\n",
    "\n",
    "This tutorial demonstrates how to create agricultural field boundaries (polygons) using [Fields of The World (FTW)](https://fieldsofthe.world/) and then use those boundaries in downstream agriculture monitoring use cases.\n",
    "\n",
    "FTW uses a [semantic segmentation](https://huggingface.co/docs/transformers/en/tasks/semantic_segmentation) model to predict where agricultural fields and their boundaries are in [Sentinel-2](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2) satellite images, which are then polygonized into individual objects (i.e., the raster predictions are turned into vector polygons). You can read more about the model and how it was trained in [Kerner et al. (2025)](https://arxiv.org/abs/2409.16252).\n",
    "\n",
    "The FTW command line interface (CLI) makes it easy to get the data needed to run the FTW model and prepare field boundary polygons for downstream use cases. In this section, we'll teach you how to create field boundaries for anywhere in the world!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoF-BxSM5Jkc"
   },
   "source": [
    "## ‚¨áÔ∏è Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qh5FKi6Nopbr"
   },
   "source": [
    "### üìç Specify your region of interest (ROI)\n",
    "Sentinel-2 data uses the MGRS (Military Grid Reference System) tiling system, meaning Sentinel-2 observations are organized by their MGRS Tile ID. This makes the MGRS Tile ID a convenient way to specify an ROI for FTW models that use Sentinel-2 inputs.\n",
    "\n",
    "In the cell below, you can select or specify the MGRS Tile ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b0f9c1cf2ba64644b3b592e6bd508200",
      "d508e36991b14b52a4c2d193a0098694"
     ]
    },
    "id": "2vzeCakqqKpB",
    "outputId": "7941636b-e6f3-4c2b-f2a0-0c61101f8638",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Either manually set a MGRS tile id here or select one on the map\n",
    "tile_id = \"14TPN\"\n",
    "utils.pick_mgrs_tile(tile_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mhUuN99qKpB",
    "outputId": "7d70b876-94e5-4d5b-ab0a-a1b5edf49f51"
   },
   "outputs": [],
   "source": [
    "tile_id = utils.get_selected_tile_id()\n",
    "print(f\"Selected tile ID: {tile_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeFZ1ZgPqKpB"
   },
   "source": [
    "### ‚è∞ Specify your time of interest (TOI)\n",
    "\n",
    "Field boundaries at the same location may change from year to year due to the dynamic nature of crop cultivation and land cover/land use change globally. Many regions have multiple growing seasons too (e.g., summer or winter).\n",
    "\n",
    "In the cell below, you will specify the year, and season to run FTW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSt6h_Q-oqjK"
   },
   "outputs": [],
   "source": [
    "year = 2023\n",
    "\n",
    "# Choose winter or summer season\n",
    "# season = \"winter\"\n",
    "season = \"summer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OENzKgXqKpB"
   },
   "source": [
    "One MGRS tile covers a very large area: 100 km x 100 km. This can take a while to download and process, so to speed up computation for this tutorial, we'll restrict our ROI to a relatively small area (1 degree x 1 degree) in the center of the specified MGRS tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5uy3Jzm8pCu"
   },
   "outputs": [],
   "source": [
    "lat, lon = MGRS().toLatLon(tile_id + \"5000050000\")  # This gets the center of the MGRS tile\n",
    "tile_center = Point(lon, lat)\n",
    "\n",
    "# Only download the imagery in a relatively small window around the tile center to speed up processing\n",
    "buffer = 0.1\n",
    "bbox_string = f\"{lon - buffer},{lat - buffer},{lon + buffer},{lat + buffer}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ul1ThvVC8pCu"
   },
   "source": [
    "### üóìÔ∏è Get the start and end of season dates from crop calendar\n",
    "\n",
    "FTW uses Sentinel-2 images from the beginning and end of an agricultural season to identify boundaries (called \"Window A\" and \"Window B\"). Since crop growth is changing between these two times, the contrasting views of the scenes help identify which parcels are agricultural (as opposed to other, less dynamic land cover types like grassland) and delineate the boundaries between fields that might not be obvious in a single scene.\n",
    "\n",
    "[Crop calendars](https://ipad.fas.usda.gov/ogamaps/cropcalendar.aspx) specify when different crop cultivation stages are happening for a given location and season. In this tutorial, we'll use the [global crop calendars from the ESA WorldCereal project](https://github.com/ucg-uv/research_products/) to define our two windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JG-9IxC8pCu",
    "outputId": "57f7c2d4-f18a-494d-c885-d3cc401d93c9"
   },
   "outputs": [],
   "source": [
    "utils.download_crop_calendars(\"data\")\n",
    "\n",
    "start_tif = os.path.join(\"data\", utils.crop_calendar_files[season][\"start\"])\n",
    "end_tif = os.path.join(\"data\", utils.crop_calendar_files[season][\"end\"])\n",
    "\n",
    "start_date, end_date = utils.get_dates_from_tifs(\n",
    "    point=tile_center,\n",
    "    start_season_tif_path=start_tif,\n",
    "    end_season_tif_path=end_tif,\n",
    "    year=year,\n",
    "    season_type=season,\n",
    ")\n",
    "\n",
    "print(f\"Season start date: {start_date}\")\n",
    "print(f\"Season end date: {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Mc7daYT8pCu",
    "outputId": "b75840b5-857b-40ef-ed11-608e69843cad"
   },
   "outputs": [],
   "source": [
    "win_a_start, win_a_end, win_b_start, win_b_end = utils.calculate_window_dates(start_date, end_date)\n",
    "\n",
    "print(f\"Window A start date: {win_a_start}\")\n",
    "print(f\"Window A end date: {win_a_end}\")\n",
    "print(f\"Window B start date: {win_b_start}\")\n",
    "print(f\"Window B end date: {win_b_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysdl442M8pCu"
   },
   "source": [
    "### ü™ü Download Sentinel-2 images for Window A and Window B\n",
    "\n",
    "Next, we need to select which Sentinel-2 images we'll use within each Window. The cell below searches for the best (least cloudy, most valid data) images within each window in the Sentinel-2 data catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sl3BpjDa8pCu",
    "outputId": "bf227926-f8cd-4ac4-9306-e2583965d9d2"
   },
   "outputs": [],
   "source": [
    "# Requesting the STAC Items for the best images\n",
    "win_a, win_b = utils.get_best_images(\n",
    "    win_a_start, win_a_end, win_b_start, win_b_end, s2_tile_id=tile_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBp5f-mjqKpC"
   },
   "source": [
    "Here's a preview of the selected images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSOwAzm3qKpC",
    "outputId": "fc23f1a7-7847-4116-ac89-4fb9889eb304"
   },
   "outputs": [],
   "source": [
    "utils.show_previews(win_a, win_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFv3iMlSqKpC"
   },
   "source": [
    "After finding the two Sentinel-2 scenes to use, we can download them and assemble them in FTW input format using the FTW CLI. The input format to FTW models contains the two 4-band (red, green, blue, near-infrared) images stacked together. This gives us an 8-band TIF raster.\n",
    "\n",
    "Warning: When running the cell below, you may get the error `\"Processing version of imagery differs. Exiting.\"` This is a known issue with the Planetary Computer catalog search that is being fixed in an upcoming release of the `ftw-tools` package. We will update the tutorial when this is fixed. If you get this error, try picking a different tile or earlier year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z2wyBW388pCu",
    "outputId": "834eb64c-f10f-4a23-df86-a312fbc5b1b1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_filename = f\"data/ftw_input_{tile_id}_{season}_{year}.tif\"\n",
    "\n",
    "!ftw inference download --win_a {win_a.id} --win_b {win_b.id} --out {image_filename} --overwrite --bbox {bbox_string}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWXsiZ5freeTG"
   },
   "source": [
    "## üõ∞Ô∏è Predict field boundaries\n",
    "\n",
    "Now that we have our input imagery, we can use a pretrained FTW model to segment the field boundaries. FTW provides [multiple pretrained models](https://github.com/fieldsoftheworld/ftw-baselines/releases) to choose from depending on your usage needs.\n",
    "\n",
    "For example, suppose your use case is commercial and you want to avoid using models pretrained on data with non-commercial licenses. In that case, you can choose a version of the FTW model pretrained only on open (CCBY) datasets.\n",
    "\n",
    "Here, we'll use the standard pretrained FTW model that uses all datasets in the benchmark and predicts 3 classes (background, boundary, and field interior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmtwKTSLqKpC"
   },
   "source": [
    "Next we use the pretrained model to predict the boundaries for our Sentinel-2 input.\n",
    "\n",
    "If you are running this notebook on a machine that has a GPU, you can add `--gpu 0` to speed up computation. Also, you can add the flag `--mps_mode` if you're running on a Mac that has a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6CeKQHfyrU7k",
    "outputId": "6ae1f4c2-6b94-41ee-d376-36610fb16776"
   },
   "outputs": [],
   "source": [
    "output_filename = f\"data/ftw_predictions_{tile_id}_{season}_{year}.tif\"\n",
    "\n",
    "if torch.backends.mps.is_available():  # Check that MPS is available\n",
    "    print(\"Running inference with MPS mode enabled\")\n",
    "    !ftw inference run {image_filename} --out {output_filename} --gpu 0 --mps_mode --model FTW_PRUE_EFNET_B3_CCBY --overwrite\n",
    "elif torch.cuda.is_available():  # check if GPU available\n",
    "    print(\"Running inference with GPU enabled\")\n",
    "    !ftw inference run {image_filename} --out {output_filename} --gpu 0 --model FTW_PRUE_EFNET_B3_CCBY --overwrite\n",
    "else:  # use CPU mode\n",
    "    print(\"Running inference with CPU only mode\")\n",
    "    !ftw inference run {image_filename} --out {output_filename} --model FTW_PRUE_EFNET_B3_CCBY --overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EF5wuDOx8pCv"
   },
   "source": [
    "### üè† Filter predicted by land cover\n",
    "FTW models are known to make some commission errors, segmenting non-crop land cover (like pasture) as \"fields\". We can try to filter out some of these erroneous predictions using an existing land cover map.\n",
    "\n",
    "This function uses the Impact Observatory landcover map by default, but you can also use ESA WorldCover by setting the `--collection_name` argument. If you'd like to see the cropland mask from the land cover map that's being used by the FTW CLI to filter the polygons, you can set the `--save_lulc_tif` flag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ftw inference filter-by-lulc --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DO_AQdOwAG2h"
   },
   "outputs": [],
   "source": [
    "filtered_output_filename = f\"ftw_predictions_filtered_{tile_id}_{season}_{year}.tif\"\n",
    "\n",
    "!ftw inference filter-by-lulc {output_filename} --out {filtered_output_filename} --overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7W6n-SSJ8pCv"
   },
   "source": [
    "### üü´ Polygonize boundaries\n",
    "\n",
    "The final step is to convert the (raster-format) predictions of where the field boundaries are to the (vector-format) field boundary polygons.\n",
    "\n",
    "Here are some additional options you can use when polygonizing boundaries, such as simplifying the geometries, filtering out minimum/maximum size, or removing holes within polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ftw inference polygonize --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fgteeBps8pCv",
    "outputId": "d2ff05e4-8f26-4c08-c250-aa7d8a7cb363"
   },
   "outputs": [],
   "source": [
    "boundaries_filename = f\"data/ftw_boundaries_{tile_id}_{season}_{year}.gpkg\"\n",
    "\n",
    "# Make sure to change {filtered_output_filename} to {output_filename}\n",
    "# if you skipped the land cover filtering step\n",
    "!ftw inference polygonize {filtered_output_filename} --out {boundaries_filename} -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0Nxy5qKqKpD"
   },
   "outputs": [],
   "source": [
    "field_boundaries = gpd.read_file(boundaries_filename)\n",
    "print(f\"Number of field boundaries: {len(field_boundaries)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Convenience function\n",
    "\n",
    "We've wrapped all of the above steps into a convenience function called `download_and_run` in the `utils.py` file that we will use for the rest of the tutorial. You can use this function to generate field boundaries for any MGRS tile, season, and year.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EPIqeh9qKpG"
   },
   "source": [
    "### üó∫Ô∏è Visualize the final field boundaries\n",
    "\n",
    "Now that we've finished creating our field boundary polygon dataset, we can visualize them on the map to see how they look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621,
     "referenced_widgets": [
      "68daeec5caee47b286be0014c6b9c331",
      "df70cc7c943e4cf0abc0a4940ba359d6",
      "b2cb48041a5c487a9982b0845a262a05",
      "94d6f4821a264372829118dbfcd72f32",
      "6ece3e85505a4865a3875c34d06ec3f4",
      "8bde923eb9774995921d02c665ed8ae3",
      "f937b98457df4566b107b2385b937c5a",
      "1530a436c1f24af8a4ddc36aaf7d09c7",
      "628fc3d190754ff78c9b0815006cbb20",
      "dde677f707a14f9db2050d67b8280326",
      "503edf84c81c4621a58074235508e7da",
      "a92f9cc948ea492d9e9fa30c2ab756d6",
      "3f94b7b7d8c54adc8fcc2d7bae5b3f8e",
      "e8e343d1453f48e1b3c1ecfa0857c9df",
      "7591024df5784f72b6ccbc91edf20149",
      "9ece9420004f4e62bc65d1ebb7db647f",
      "35eaa74ecbba417d99a800a8f0015e30",
      "1cb15a5477294ad291473eda4d91f2fd",
      "ec2cf26facb241daaa5428400860b7ef",
      "1c0067196f3f46e49fb2d79c82ea32c9",
      "1c7485a9e8554a15b6b267feaef3ea24",
      "5e903a5b58fa4f6c834d5aba4bcb6801",
      "3174b3a7ee714e00bfe764094836511d",
      "e318d122a40944ad844456b507081985",
      "e274a16067a94e1085125c83930425be",
      "6684abdbb98745b58e12474998bcab17",
      "0f14e59ad0b140bbb73668be901f22c4",
      "5cdd6b6876de4ca8b468b07c77dd934b",
      "211f14bf1af349d2a0d8f0c5c5896409",
      "081ae56dc2aa4e83a66870e57fc508a7",
      "47ee499818504bcfaf322d1187aa87ec",
      "0d7223e418df4bc7ac82eb4bef91b5ba",
      "f8f699a8fbdf4bd39be0fecc80e3d443",
      "cad21fc043094e4995d6af465345c6f6",
      "00bd592e59d345edb2f94373d2146d32",
      "a398f19da2174f5aaf84fa5a06f5be1b",
      "5a310c52bf0545a2a7e919aada299a32",
      "ca0a34f258a9488998f65961cd0ecfb5",
      "e858ef7311f34ba58376cc2974913e29",
      "3d66b9cc3af2454ba9a5018278807bcc",
      "1f7c495e8db34ba5af70f993f9b999d9",
      "acb155c6d808410ca6691c418d617593",
      "d2b6322ee56d464b95cf1bb254c18bba",
      "63e19240218b4ad5b27a69fb95747ac6",
      "fb3cf153b4aa4312b6d099c87d5ca43b",
      "588a1de82a684f168170079be53cfda8",
      "c01fc90221804f26b67089501d10e09e",
      "d5e4656fbef34102b3b6cbb3d3c5666c",
      "a93039bc97f9427b9c4c83b693359418",
      "27840a4afdb34c2e900cb0a3b2d9717f",
      "a6d8b23fa8174f5bbbbc82213a8155c5",
      "861adc9867bb41fc86bff63d12532a7f",
      "9a957d1cc8774e8290dd1f175beef61d",
      "81510918e4674a81bf79ef5376d605a5",
      "b429ea6a688345e38d296df5deb10c5a",
      "b393923ec9b64f97b5a639d647c8b43c",
      "aa373c9c4a4047338b5f937834e6bb4f",
      "759860d88dcf41e3a193a86ea39ddbe3",
      "b63b027647ec4002b37b4401850c9a7e",
      "8a4f0dd686274dd9ab9e88a8243ffe25"
     ]
    },
    "id": "kTs_A6b_8pCv",
    "outputId": "5e2b49c5-228b-408e-8a15-2533c1167c8d"
   },
   "outputs": [],
   "source": [
    "if \"google.colab\" in str(get_ipython()):\n",
    "    from google.colab import output\n",
    "\n",
    "    output.enable_custom_widget_manager()\n",
    "\n",
    "GOOGLE_MAPS_XYZ = \"https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}\"\n",
    "\n",
    "m = leafmap.Map(\n",
    "    center=[lat, lon],\n",
    "    zoom=12,\n",
    "    draw_control=False,\n",
    "    measure_control=False,\n",
    "    fullscreen_control=False,\n",
    "    attribution_control=True,\n",
    ")\n",
    "m.add_tile_layer(url=GOOGLE_MAPS_XYZ, name=\"Google Satellite\", attribution=\"Google\")\n",
    "m.add_geojson(field_boundaries, layer_name=\"Generated Fields\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qa9Iuu2GU52Z"
   },
   "source": [
    "<a name=\"methodology\"></a>\n",
    "# ‚öô Methodology\n",
    "\n",
    "In this section, we will demonstrate how to use FTW field boundaries in two different use cases of agricultural monitoring under climate change:  \n",
    "1. üåæ Crop type mapping in Iowa, United States  \n",
    "2. üå≥ Forest loss monitoring in Brazil  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_caOjqFAiZ3"
   },
   "source": [
    "## üåæ Use case: crop type mapping with few labels\n",
    "\n",
    "Crop type mapping involves classifying agricultural fields into categories such as maize, wheat, soy, or rice, usually using satellite imagery and machine learning. A key challenge is that ground truth labels (farmer surveys, government records, or field visits) are scarce, expensive to collect, and unevenly distributed across the globe. This is especially true in regions where data infrastructure is weak but where climate change impacts on agriculture are most severe (e.g., Sub-Saharan Africa, parts of South Asia, Amazonia).\n",
    "\n",
    "Few-label or weakly supervised methods allow us to train crop classification models using very limited labeled data, combined with large volumes of unlabeled satellite imagery. Techniques like transfer learning, semi-supervised learning, or foundation models can help enable accurate mapping even in data-sparse regions.\n",
    "\n",
    "In this tutorial, we'll demonstrate how you can use pretrained/precomputed embeddings to support crop type classification with few labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6U3G-q0qKpH"
   },
   "source": [
    "### üü© Generate boundaries for crop type mapping use case\n",
    "\n",
    "Let's generate the boundaries for a tile in Iowa (14TPN) in 2023 during the summer growing season. These are the default settings from the previous set of cells, but if you are just picking up here, then you can run the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = utils.download_and_run(\"14TPN\", \"summer\", 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_boundaries = gpd.read_file(results[\"boundaries_filename\"])\n",
    "image_filename = results[\"image_filename\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8L28axZ4NVj"
   },
   "source": [
    "### üß† Generate embeddings for each field\n",
    "\n",
    "We can generate per-field embeddings using a variety of methods, from simple statistics over the input channels, to more complex methods such as using pre-trained deep learning models.\n",
    "\n",
    "Geospatial embeddings offer end-users a more efficient way of mapping with Earth observation data. They provide a compressed feature representation that can be used as input to lightweight models like logistic regression or random forests for classification, instead of higher-dimensional raw satellite observations that are more difficult to learn patterns from.\n",
    "\n",
    "In this tutorial, we'll explore [MOSAIKS](https://www.nature.com/articles/s41467-021-24638-z) embeddings. MOSAIKS uses random convolutional filters to generate embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSeRzl7bAwgR"
   },
   "source": [
    "#### üé® MOSAIKS\n",
    "\n",
    "The MOSAIKS method uses random convolutional features to generate embeddings for each field. MOSAIKS was introduced in [A generalizable and accessible approach to machine learning with global satellite imagery](https://www.nature.com/articles/s41467-021-24638-z)$^1$ and consists of running a set of random convolutional filters over the input channels and pooling the resulting feature maps into an embedding vector. This method is fast and generates embeddings that are useful for a wide variety of downstream tasks.\n",
    "\n",
    "The cells below create random convolutional filters and then apply them to generate feature embeddings for the Sentinel-2 pixels associated with each field polygon.\n",
    "\n",
    "Notice how this method is extremely fast!\n",
    "\n",
    "$^1$ Rolf, Esther, et al. \"A generalizable and accessible approach to machine learning with global satellite imagery.\" Nature communications 12.1 (2021): 4392."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6HotiSW4Pe8"
   },
   "outputs": [],
   "source": [
    "from utils import RCFWithCustomMaskPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZkXoT4B18pCz"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDhGJJUF8pCz"
   },
   "outputs": [],
   "source": [
    "model = RCFWithCustomMaskPooling(\n",
    "    in_channels=8, features=256, kernel_size=3, seed=0, mode=\"gaussian\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IduxI3b8pCz",
    "outputId": "11938ce2-8326-4f78-f92a-022a027cb3f5"
   },
   "outputs": [],
   "source": [
    "PAD_WIDTH = 3  # this ensures that each crop is at least 3 pixels\n",
    "\n",
    "feature_per_field = []\n",
    "with rasterio.open(image_filename) as f:\n",
    "    for shape in tqdm(field_boundaries.geometry):\n",
    "        out_image, out_transform = rasterio.mask.mask(\n",
    "            f,\n",
    "            [shape],\n",
    "            crop=True,\n",
    "            all_touched=True,\n",
    "            nodata=0,\n",
    "            pad=True,\n",
    "            pad_width=PAD_WIDTH,\n",
    "        )\n",
    "        field_mask = out_image[0] > 0\n",
    "\n",
    "        out_image, out_transform = rasterio.mask.mask(\n",
    "            f,\n",
    "            [shape.envelope],\n",
    "            crop=True,\n",
    "            all_touched=True,\n",
    "            nodata=0,\n",
    "            pad=True,\n",
    "            pad_width=PAD_WIDTH,\n",
    "        )\n",
    "\n",
    "        num_channels, height, width = out_image.shape\n",
    "\n",
    "        assert height >= PAD_WIDTH\n",
    "        assert width >= PAD_WIDTH\n",
    "        with torch.inference_mode():\n",
    "            features = model.forward_masked(\n",
    "                torch.tensor(out_image / 3000.0).float().to(device),\n",
    "                torch.tensor(field_mask).to(device),\n",
    "            )\n",
    "            feature_per_field.append(features.cpu().numpy())\n",
    "features_per_field = np.array(feature_per_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyqlYdGjqKpH"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca = PCA(n_components=3, whiten=True, random_state=0)\n",
    "features_per_field_scaled = pca.fit_transform(features_per_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BQ8hbZM8pCv"
   },
   "source": [
    "### üßëüèΩ‚Äçüåæ Join with the USDA's Cropland Data Layer (CDL) data\n",
    "\n",
    "The Cropland Data Layer (CDL) is an annual, raster-based land cover dataset produced by the USDA National Agricultural Statistics Service (NASS). It is generated using satellite imagery (primarily Landsat, Sentinel-2, and other sensors) combined with ground-truth data from NASS surveys and the Farm Service Agency (FSA). Each pixel in the CDL is classified into a specific crop type or land cover class at 30 m (and more recently 10 m) spatial resolution, using supervised machine learning algorithms such as decision trees.\n",
    "\n",
    "Note that CDL is only available in the continental United States.\n",
    "\n",
    "For the purposes of this tutorial we use the CDL data as a source of labels for our crop type mapping task by assuming that the majority class within each field polygon is the crop type for that field. While this is a simplification, the key aspect is that we can perform spatial joins between field boundaries and other spatial datasets to serve as the basis of downstream analysis.\n",
    "\n",
    "The cells below download the CDL raster for our year of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4c5PF_p8pCv"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchgeo.datasets import CDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-PB3dI88pCv"
   },
   "outputs": [],
   "source": [
    "# TorchGeo provides a handy way to download and access the CDL dataset\n",
    "_ = CDL(paths=\"data/cdl\", years=[2023], download=True)\n",
    "zip_path = \"data/cdl/2023_30m_cdls.zip\"\n",
    "if os.path.exists(zip_path):\n",
    "    os.remove(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObcdRcib8pCv"
   },
   "outputs": [],
   "source": [
    "# warp the fields to the CDL CRS\n",
    "with rasterio.open(\"data/cdl/2023_30m_cdls.tif\") as cdl_ds:\n",
    "    field_boundaries_warped = field_boundaries.to_crs(cdl_ds.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upJMhbMj8pCv"
   },
   "outputs": [],
   "source": [
    "# check to make sure the geometries aren't inf -- if there is a warping issue in the gdal layer then geopandas will fail silently\n",
    "assert np.all(field_boundaries_warped.geometry.is_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWgEBE7FqKpI"
   },
   "source": [
    "The below code gets the crop type category of each field boundary polygon based on the majority class in the CDL raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "waTIsF7U8pCv",
    "outputId": "28cddb44-6d2d-4ed4-ecbd-d3e3181539e4"
   },
   "outputs": [],
   "source": [
    "majority_class_vals = []\n",
    "majority_class_percents = []\n",
    "with rasterio.open(\"data/cdl/2023_30m_cdls.tif\") as f:\n",
    "    for geom in tqdm(field_boundaries_warped.geometry.values):\n",
    "        out_image, out_transform = rasterio.mask.mask(\n",
    "            f,\n",
    "            [geom],\n",
    "            crop=True,\n",
    "            all_touched=True,\n",
    "            nodata=0,\n",
    "        )\n",
    "        out_image = out_image.squeeze().flatten()\n",
    "        out_image = out_image[out_image != 0]\n",
    "        N = out_image.shape[0]\n",
    "        vals, counts = np.unique(out_image, return_counts=True)\n",
    "        counts = counts / N\n",
    "        val_counts = dict(zip(vals, counts, strict=False))\n",
    "        max_val = max(val_counts, key=val_counts.get)\n",
    "        majority_class_vals.append(max_val)\n",
    "        majority_class_percents.append(val_counts[max_val])\n",
    "\n",
    "majority_class_vals = np.array(majority_class_vals)\n",
    "majority_class_percents = np.array(majority_class_percents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6-zLRskqKpI"
   },
   "source": [
    "Next, we encode the label categories (which are strings) into a format that can be used by Scikit-learn classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0JVfj2VqKpI"
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "majority_class_vals_squashed = encoder.fit_transform(majority_class_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9H041c6RqKpI"
   },
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    utils.CDL_CODE_TO_NAME[val]\n",
    "    for val in encoder.inverse_transform(np.unique(majority_class_vals_squashed))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzcOFCIHqKpI"
   },
   "source": [
    "In the cell below, we visualize the embeddings (in the reduced-dimension principal component subspace) colored by crop type class.\n",
    "\n",
    "This gives us an idea of how separable the crop type categories are in our input datasets. You can see that some classes like alfalfa are well-separated from other classes, while classes like corn and soybean are clustered together due to their similar crop phenologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YQYOm3nqKpI",
    "outputId": "bedb5e47-6d03-41ff-b8d7-3af7181b2fea"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "cmap = plt.cm.tab20\n",
    "norm = mpl.colors.Normalize(\n",
    "    vmin=majority_class_vals_squashed.min(), vmax=majority_class_vals_squashed.max()\n",
    ")\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    features_per_field_scaled[:, 0],\n",
    "    features_per_field_scaled[:, 1],\n",
    "    c=majority_class_vals_squashed,\n",
    "    s=8,\n",
    "    cmap=cmap,\n",
    "    alpha=0.9,\n",
    "    norm=norm,\n",
    ")\n",
    "\n",
    "handles = []\n",
    "unique_classes = np.unique(majority_class_vals_squashed)\n",
    "for i, val in enumerate(unique_classes):\n",
    "    handles.append(\n",
    "        plt.Line2D(\n",
    "            [0],\n",
    "            [0],\n",
    "            marker=\"o\",\n",
    "            color=\"w\",\n",
    "            label=class_names[i],\n",
    "            markerfacecolor=cmap(norm(val)),\n",
    "            markersize=8,\n",
    "        )\n",
    "    )\n",
    "\n",
    "plt.legend(\n",
    "    handles=handles,\n",
    "    title=\"Class\",\n",
    "    bbox_to_anchor=(1.05, 1),\n",
    "    loc=\"upper left\",\n",
    "    borderaxespad=0.0,\n",
    "    frameon=False,\n",
    "    title_fontsize=\"large\",\n",
    ")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.title(\"PCA of Features per Field\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7L0sWlLo8pCz"
   },
   "source": [
    "### üåΩ Predicting crop type with few labels\n",
    "\n",
    "Now that we have a feature embedding for each field, we can use those in all types of downstream tasks. Below we show how to use the embeddings to predict crop type with few labels. We will use the CDL data as a source of labels, but we can also use other sources of labels such as field surveys or expert knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCh9pJYe8pCz"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1z63o8C28pCz"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(\n",
    "    features_per_field: list[np.ndarray],\n",
    "    majority_class_vals: list[int],\n",
    "    min_fields: int = 10,\n",
    "    train_fraction: float = 0.1,\n",
    "    random_state: int = 0,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, LabelEncoder]:\n",
    "    \"\"\"Prepare and split the dataset into train/test sets while filtering out classes\n",
    "    that have too few examples to consider.\"\"\"\n",
    "    vals, counts = np.unique(majority_class_vals, return_counts=True)\n",
    "    field_class_counts = dict(zip(vals, counts, strict=False))\n",
    "\n",
    "    # Filter out underrepresented classes\n",
    "    x_all, y_all = [], []\n",
    "    for x, y in zip(features_per_field, majority_class_vals, strict=False):\n",
    "        if field_class_counts[y] < min_fields:\n",
    "            continue\n",
    "        x_all.append(x)\n",
    "        y_all.append(int(y))\n",
    "\n",
    "    x_all = np.array(x_all)\n",
    "    y_all = np.array(y_all)\n",
    "\n",
    "    # Encode labels\n",
    "    encoder = LabelEncoder()\n",
    "    y_all = encoder.fit_transform(y_all)\n",
    "\n",
    "    # Train/test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_all,\n",
    "        y_all,\n",
    "        test_size=1 - train_fraction,\n",
    "        random_state=random_state,\n",
    "        stratify=y_all,\n",
    "    )\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYjHwtcJ8pC0"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(\n",
    "    x_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    x_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    random_state: int = 0,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Scale data, train Logistic Regression, and return accuracy.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "    model = LogisticRegression(C=1.0, max_iter=1000, random_state=random_state, tol=1e-12)\n",
    "    model.fit(x_train_scaled, y_train)\n",
    "\n",
    "    y_pred = model.predict(x_test_scaled)\n",
    "    return y_pred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W63-uF0l8pC0"
   },
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    features_per_field: list[np.ndarray],\n",
    "    majority_class_vals: list[int],\n",
    "    min_fields: int = 10,\n",
    "    train_fraction: float = 0.1,\n",
    "    random_state: int = 0,\n",
    "    verbose: bool = True,\n",
    ") -> tuple[np.ndarray, np.ndarray, list[str]]:\n",
    "    \"\"\"Run the full experiment and print summary stats.\"\"\"\n",
    "    x_train, x_test, y_train, y_test, encoder = prepare_dataset(\n",
    "        features_per_field,\n",
    "        majority_class_vals,\n",
    "        min_fields=min_fields,\n",
    "        train_fraction=train_fraction,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    train_counts = dict(\n",
    "        zip(\n",
    "            encoder.inverse_transform(np.unique(y_train, return_counts=True)[0]),\n",
    "            np.unique(y_train, return_counts=True)[1],\n",
    "            strict=False,\n",
    "        )\n",
    "    )\n",
    "    test_counts = dict(\n",
    "        zip(\n",
    "            encoder.inverse_transform(np.unique(y_test, return_counts=True)[0]),\n",
    "            np.unique(y_test, return_counts=True)[1],\n",
    "            strict=False,\n",
    "        )\n",
    "    )\n",
    "    class_names = [\n",
    "        utils.CDL_CODE_TO_NAME[val] for val in encoder.inverse_transform(np.unique(y_test))\n",
    "    ]\n",
    "    if verbose:\n",
    "        print(f\"Train size: {len(y_train)}, Test size: {len(y_test)}\")\n",
    "        for val in test_counts:\n",
    "            print(\n",
    "                f\"{utils.CDL_CODE_TO_NAME[val]} ‚Äî train: {train_counts[val]}, test: {test_counts[val]}\"\n",
    "            )\n",
    "\n",
    "    y_pred, y_test = train_and_evaluate_model(\n",
    "        x_train, y_train, x_test, y_test, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return y_pred, y_test, class_names, encoder, train_counts, test_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ypDo6Th78pC0",
    "outputId": "4342f7d5-f686-4bce-ff72-e8861b64a015"
   },
   "outputs": [],
   "source": [
    "y_pred, y_test, class_names, encoder, train_counts, test_counts = run_experiment(\n",
    "    features_per_field=features_per_field,\n",
    "    majority_class_vals=majority_class_vals,\n",
    "    min_fields=10,\n",
    "    train_fraction=0.1,\n",
    "    random_state=0,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJBVXfD1DcR9"
   },
   "source": [
    "### ü´õ Crop type mapping: Results & Discussion\n",
    "\n",
    "The cells below compute the confusion matrix and other key performance metrics for our model applied to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "id": "VYIvQ3k78pC0",
    "outputId": "57d9f176-6576-4641-e050-298173ee9a08"
   },
   "outputs": [],
   "source": [
    "cnf = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cnf, display_labels=class_names)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "disp.ax_.set_title(\"Confusion Matrix\")\n",
    "disp.ax_.set_xlabel(\"Predicted\")\n",
    "disp.ax_.set_xticklabels(class_names, rotation=90)\n",
    "disp.ax_.set_ylabel(\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "F1mH0NYf8pC0",
    "outputId": "c35f8a8a-bf99-447e-db3d-1c869d1b8c22"
   },
   "outputs": [],
   "source": [
    "per_class_precision = precision_score(y_test, y_pred, average=None, zero_division=0)\n",
    "per_class_recall = recall_score(y_test, y_pred, average=None, zero_division=0)\n",
    "per_class_f1 = f1_score(y_test, y_pred, average=None, zero_division=0)\n",
    "\n",
    "rows = []\n",
    "for class_index in range(len(class_names)):\n",
    "    class_name = class_names[class_index]\n",
    "\n",
    "    class_num = encoder.inverse_transform([class_index])[0]\n",
    "    n_train = train_counts[class_num]\n",
    "    n_test = test_counts[class_num]\n",
    "\n",
    "    precision = per_class_precision[class_index]\n",
    "    recall = per_class_recall[class_index]\n",
    "    f1 = per_class_f1[class_index]\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"class_name\": class_name,\n",
    "            \"n_train\": n_train,\n",
    "            \"n_test\": n_test,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "    )\n",
    "pd.DataFrame(rows).sort_values(by=\"f1\", ascending=False).reset_index(drop=True).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGN-2iX18pC0"
   },
   "source": [
    "Here are some observations we can make about our results:  \n",
    "- üåæ For the crop classes: Soybeans, Corn, Alfalfa we see F1 scores between 0.63 and 0.68. While we would like these scores to be higher in an operational application, these scores aren't bad for a model that used relatively few (~30) labeled examples per class!  \n",
    "- üå± Looking at the confusion matrix, we can see that the model is struggling to distinguish between Soybeans and Corn. This makes sense because these crops have similar appearances and phenologies in remote sensing observations.  \n",
    "- üêÑ The model has some confusion between Grass/Pasture and Alfalfa, which is not surprising since growers often plant Alfalfa in Pasture as forage for animals.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vLLeGTM8pC0"
   },
   "source": [
    "### üìà How does performance change with more labels?\n",
    "\n",
    "If you're thinking about how to improve the performance of your classifier, a natural next step might be to add more training labels. In the below cells, we investate how adding more training data improves the performance for the corn and soybean classes, which the model struggled with in our initial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2XNpHFB8pC0",
    "outputId": "c68f1a04-a854-463e-d3a7-1607b9f7e143"
   },
   "outputs": [],
   "source": [
    "# run experiments with varying amount of training data then plot the f1 score for corn and soybeans over training fraction\n",
    "xs = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "ys_corn = []\n",
    "ys_soybean = []\n",
    "for pct in xs:\n",
    "    corn_f1s = []\n",
    "    soybean_f1s = []\n",
    "    for seed in tqdm(range(10), desc=f\"Running experiment with train fraction {pct}\"):\n",
    "        y_pred, y_test, class_names, encoder, train_counts, test_counts = run_experiment(\n",
    "            features_per_field=features_per_field,\n",
    "            majority_class_vals=majority_class_vals,\n",
    "            min_fields=10,\n",
    "            train_fraction=pct,\n",
    "            random_state=seed,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        per_class_f1 = f1_score(y_test, y_pred, average=None, zero_division=0)\n",
    "        corn_f1 = per_class_f1[class_names.index(\"Corn\")]\n",
    "        soybean_f1 = per_class_f1[class_names.index(\"Soybeans\")]\n",
    "        corn_f1s.append(corn_f1)\n",
    "        soybean_f1s.append(soybean_f1)\n",
    "    ys_corn.append(corn_f1s)\n",
    "    ys_soybean.append(soybean_f1s)\n",
    "\n",
    "xs = np.array(xs)\n",
    "ys_corn = np.array(ys_corn)\n",
    "ys_soybean = np.array(ys_soybean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nYJqFbS8pC0",
    "outputId": "9778ef91-16c4-4abd-97d6-36b49d687358"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "plt.errorbar(xs, ys_corn.mean(axis=1), yerr=ys_corn.std(axis=1), label=\"Corn\", marker=\"o\")\n",
    "plt.errorbar(\n",
    "    xs,\n",
    "    ys_soybean.mean(axis=1),\n",
    "    yerr=ys_soybean.std(axis=1),\n",
    "    label=\"Soybeans\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "plt.xlabel(\"Training Fraction\", fontsize=12)\n",
    "plt.ylabel(\"F1 Score\", fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(xs, [f\"{x * 100:.0f}%\" for x in xs], fontsize=12)\n",
    "plt.legend(fontsize=14)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7J7gh_CfqKpJ"
   },
   "source": [
    "The results from this experiment show that increasing the amount of training labels indeed does improve performance for the Corn and Soybean classes. However, the benefit of adding more labels starts to plateau around 30-40% of the available data. This suggests that adding more labels will have diminishing returns after this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ5IkSICCahq"
   },
   "source": [
    "## üå≥ Use case: monitoring forest loss\n",
    "\n",
    "Deforestation is a major driver of greenhouse gas emissions and biodiversity loss. Many agricultural commodities like soy, palm oil, and cocoa are linked to deforestation. Policies such as the EU Deforestation Regulation (EUDR) now require proof that products entering European markets are not associated with recent forest loss. Reliable monitoring enables compliance with such policies, supports sustainable supply chains, and helps protect forests as vital carbon sinks.\n",
    "\n",
    "In this use case, we'll demonstrate how to identify if and when forest loss occurred within agricultural field boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puA1U54ZqKpJ"
   },
   "source": [
    "### üü• Do it yourself: Generate boundaries for forest loss use case\n",
    "\n",
    "Let's generate the boundaries for a tile in Brazil (21LXF) in 2022 during the winter growing season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = utils.download_and_run(\"21LXF\", \"winter\", 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_boundaries = gpd.read_file(results[\"boundaries_filename\"])\n",
    "bbox_string = results[\"bbox_string\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4HquCqnCgcH"
   },
   "source": [
    "### ü™æ Join with Hansen Global Forest Change dataset\n",
    "\n",
    "The Hansen [Global Forest Change dataset](https://storage.googleapis.com/earthenginepartners-hansen/GFC-2022-v1.10/download.html) contains results from time-series analysis of Landsat images in characterizing global forest extent and change from 2000 through 2024.\n",
    "\n",
    "The Hansen dataset contains a layer called `lossyear`, which is defined as:\n",
    "\n",
    "> **Year of gross forest cover loss event (lossyear)** <br>\n",
    "> Forest loss during the period 2000-2024, defined as a stand-replacement disturbance, or a change from a forest to non-forest state. Encoded as either 0 (no loss) or else a value in the range 1-24, representing loss detected primarily in the year 2001-2024, respectively.\n",
    "\n",
    "We will join the `lossyear` layer with our field boundaries to analyze the average year when forest loss occurred in each field.\n",
    "\n",
    "Note: There are other layers that can be investigated for forest change monitoring. See the [documentation](https://storage.googleapis.com/earthenginepartners-hansen/GFC-2022-v1.10/download.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znkJVjczqKpJ"
   },
   "source": [
    "The Hansen dataset is stored in 1x1 degree tiles. We will use our bounding box coordinates to get the overlapping tile ID in the Hansen dataset, then download it for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pco4ncohDxPt",
    "outputId": "f1841d8e-a21e-49db-db90-09ae0e2feb83"
   },
   "outputs": [],
   "source": [
    "# get min, max lat and min, max lon from bbox_string\n",
    "bbox = [float(x) for x in bbox_string.split(\",\")]\n",
    "min_lon, min_lat, max_lon, max_lat = bbox\n",
    "glad_filename = utils.hansen_filenames_from_bbox(\n",
    "    min_lat, max_lat, min_lon, max_lon, layer=\"lossyear\"\n",
    ")[0]\n",
    "print(glad_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMdn6D4tDxM7"
   },
   "outputs": [],
   "source": [
    "# Download forest lossyear tif\n",
    "glad_url = utils.download_glad_granule(glad_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ifw-1cy1DxJ_"
   },
   "outputs": [],
   "source": [
    "# warp the fields to the GLAD CRS\n",
    "with rasterio.open(f\"data/{glad_filename}\") as glad_ds:\n",
    "    field_boundaries_warped = field_boundaries.to_crs(glad_ds.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEBp-XoPD2AT"
   },
   "outputs": [],
   "source": [
    "# check to make sure the geometries aren't inf -- if there is a warping issue in the gdal layer then geopandas will fail silently\n",
    "assert np.all(field_boundaries_warped.geometry.is_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUUJHInsqKpJ"
   },
   "source": [
    "Now that we've downloaded the Hansen `lossyear` raster, we can extract the pixel values within each field boundary.\n",
    "\n",
    "Since there are multiple pixels in each boundary that might have different values (e.g., if deforestation happened over multiple years), we compute the mean `lossyear` in each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j2__A_2rD3Er",
    "outputId": "069e4456-7cf4-4f99-88e3-255860029177"
   },
   "outputs": [],
   "source": [
    "forest_loss_vals = []\n",
    "with rasterio.open(f\"data/{glad_filename}\") as f:\n",
    "    # extract the lossyear values for each field boundary polygon\n",
    "    for geom in tqdm(field_boundaries_warped.geometry.values):\n",
    "        out_image, out_transform = rasterio.mask.mask(\n",
    "            f,\n",
    "            [geom],\n",
    "            crop=True,\n",
    "            all_touched=True,\n",
    "        )\n",
    "        # get the mean lossyear in each polygon\n",
    "        out_image = out_image.squeeze().flatten().mean()\n",
    "        forest_loss_vals.append(out_image)\n",
    "# add the mean lossyear as a new column in the dataframe\n",
    "forest_loss_vals = np.array(forest_loss_vals)\n",
    "field_boundaries_warped[\"mean_lossyear\"] = forest_loss_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhkmytKNU_Z2"
   },
   "source": [
    "### üå≤ Monitoring Forest Loss: Results & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ArYeaTZqKpJ"
   },
   "source": [
    "In the cell below, we'll visualize the mean forest loss year in each field polygon in our ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "zFP-4QWWD7fu",
    "outputId": "30280725-ddb5-4842-a2ca-bb2da5ddcdc9"
   },
   "outputs": [],
   "source": [
    "# Plot with 4 graduated levels\n",
    "field_boundaries_warped.plot(\n",
    "    column=\"mean_lossyear\",\n",
    "    cmap=\"magma\",\n",
    "    scheme=\"EqualInterval\",\n",
    "    k=4,  # number of intervals\n",
    "    legend=True,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LWo6UQzwj37"
   },
   "source": [
    "The color scale encodes when deforestation events occurred inside each agricultural field polygon:\n",
    "\n",
    "* Black (2000‚Äì2006) ‚Üí Little to no forest loss detected in recent decades. These fields have remained relatively stable or were cleared prior to the Hansen dataset‚Äôs baseline (2000).\n",
    "\n",
    "* Purple (2006‚Äì2012) ‚Üí Forest loss primarily concentrated in the early 2000s. These fields likely underwent agricultural expansion during this period.\n",
    "\n",
    "* Red (2012‚Äì2018) ‚Üí Forest loss in the 2010s, reflecting more recent agricultural encroachment.\n",
    "\n",
    "* Yellow (2018‚Äì2024) ‚Üí Forest loss in the early 2020s, showing active or ongoing deforestation.\n",
    "\n",
    "While many fields remain black (no recent loss), there are clear clusters of purple and red polygons, suggesting bursts of agricultural expansion at different times. This indicates staggered waves of land conversion, not a single deforestation event. The yellow polygons highlight very recent deforestation inside field boundaries, a strong indicator of active frontier expansion into forested areas.\n",
    "\n",
    "These results can directly support monitoring under initiatives like the EU Deforestation Regulation (EUDR), which requires demonstrating that agricultural products (soy, beef, etc.) are not linked to deforestation after December 2020. The highlighted fields with red and yellow values could represent non-compliant areas.\n",
    "\n",
    "Each wave of deforestation contributes to carbon emissions and biodiversity loss. Identifying the timing and location of these events is crucial for quantifying the climate impact of agricultural expansion and prioritizing interventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHIjM6eMwlXY"
   },
   "source": [
    "<a name=\"final-discussion-takeaways\"></a>\n",
    "# üéì Final Discussion & Takeaways\n",
    "\n",
    "## üìù Takeaways\n",
    "\n",
    "The exercises in this tutorial notebook demonstrated the following lessons related to the tutorial‚Äôs learning outcomes:  \n",
    "\n",
    "1. üåê **Field boundaries as a foundation for agricultural monitoring**: Generating and using field boundaries provides a common framework for linking remote sensing data to agricultural monitoring tasks.  \n",
    "2. üå≥ **Forest loss monitoring**: We saw how forest change datasets can be intersected with field polygons to identify which agricultural fields are associated with recent deforestation. This connects directly to real-world needs such as supply chain verification and compliance with regulations like the EU Deforestation Regulation.  \n",
    "3. üåæ **Crop type mapping with few labels**: This example highlighted how even limited ground-truth labels can be leveraged for crop classification, and how model accuracy scales with more training data. This is particularly relevant for regions where data scarcity is a barrier to climate monitoring.  \n",
    "4. üîÑ **End-to-end workflow practice**: By completing both exercises, users experienced the process of generating field boundaries, integrating them with open datasets, applying machine learning, and critically interpreting the results.  \n",
    "\n",
    "## ‚ö†Ô∏è Limitations\n",
    "\n",
    "### üü™ Field boundary polygons\n",
    "- üü™ FTW polygons are generated automatically using machine learning models, not hand-drawn cadastral maps.  \n",
    "- üü™ Boundaries may not always perfectly align with actual field edges, especially in regions with irregularly shaped plots or complex land-use mosaics.  \n",
    "- üü™ Polygons are derived from medium-resolution imagery (e.g., Sentinel-2 at 10 m). Smallholder plots below the resolution threshold may not be delineated or may be merged into larger polygons.  \n",
    "  - ‚ö†Ô∏è This introduces a bias toward large, industrial fields, while potentially underrepresenting smallholder farms that dominate in many Global South contexts.  \n",
    "- üü™ Model performance can vary across regions due to differences in landscape patterns, cloud cover, and availability of training data. While the FTW training dataset includes 24 countries across 4 continents (Africa, South America, Asia, and Europe), most of the training data comes from Europe.  \n",
    "  - ‚ö†Ô∏è Boundaries may be more accurate in data-rich areas (e.g., Europe) and less reliable in underrepresented regions (e.g., Sub-Saharan Africa and South America).  \n",
    "\n",
    "### üå≥ Forest loss example\n",
    "- üå≥ The Hansen Global Forest Change dataset measures **tree cover loss** at 30 m resolution but does not differentiate between permanent deforestation and temporary disturbances (e.g., fire, logging cycles).  \n",
    "- üå≥ Field boundaries may be smaller than the Hansen pixels, creating uncertainty in attributing loss to specific fields.  \n",
    "- üå≥ The dataset baseline begins in 2000, so older deforestation is not captured.  \n",
    "- üå≥ Results should therefore be interpreted as relative patterns of recent loss, not complete histories of land-use change.  \n",
    "\n",
    "### üåæ Crop type mapping example\n",
    "- üåæ The exercise with the USDA Cropland Data Layer (CDL) showed how to perform classification with few labels, but CDL itself is only available for the U.S. and may not generalize globally. \n",
    "- üåæ Model performance improves with more training labels, but in many regions outside the U.S. labeled crop data is sparse or unavailable.  \n",
    "- üåæ Crop type classification accuracy is also affected by cloud cover, sensor resolution, and crop heterogeneity within fields.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLLCQpv14Gsx"
   },
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "After completing this tutorial, readers should be able to:  \n",
    "* üåç Apply the **Fields of the World (FTW)** dataset to generate field boundaries for any location.  \n",
    "* üåæ Use those boundaries for downstream applications such as **crop type classification** and **forest loss monitoring**.  \n",
    "* üß≠ Critically assess the **strengths and limitations** of satellite-based monitoring for agriculture-related decision making.  \n",
    "\n",
    "The next steps for readers could include:  \n",
    "1. üïí **Temporal evolution of field boundaries**: Tracking how agricultural field boundaries evolve ‚Äî whether through expansion into forests, fragmentation of small plots, or consolidation into large industrial fields ‚Äî provides insight into the dynamics of land-use change under climate and economic pressures.  \n",
    "2. üìà **Scaling up analyses**: Moving from local examples to larger regional or national scales. \n",
    "3. üå± **Exploring additional crops or land-use types**: Extending classification tasks to multiple crop classes or to mixed land-use systems.  \n",
    "4. üèõÔ∏è **Integrating socio-economic data**: Combining geospatial monitoring with trade, policy, or farmer-level data to evaluate climate adaptation strategies.  \n",
    "5. ü§ù **Validating with local knowledge**: Collaborating with farmers, NGOs, or government agencies to ground-truth and improve the results.  \n",
    "6. üíª **Contributing to FTW**: FTW is an open-source project. Read more about [how you can contribute](https://fieldsofthe.world/contributing.html) to the project.  \n",
    "\n",
    "### üå°Ô∏è Other Potential Applications  \n",
    "\n",
    "The methods introduced in this tutorial can be extended to a wide range of additional use cases, including:  \n",
    "\n",
    "* üåæ **Yield forecasting**: Using crop type maps and time-series vegetation indices to estimate production under climate variability.  \n",
    "* üíß **Drought monitoring**: Assessing the impacts of water stress on specific crops or regions using field boundaries as spatial units.  \n",
    "* üåç **Greenhouse gas accounting**: Linking crop maps to emission factors for different farming systems to improve agricultural GHG inventories.  \n",
    "* üîó **Sustainable supply chain monitoring**: Verifying compliance with deforestation-free regulations (e.g., EUDR) or corporate sustainability pledges.  \n",
    "* üìä **Climate adaptation planning**: Identifying shifts in crop distributions due to temperature and rainfall changes to inform adaptation strategies.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkKqewK_-ZLD"
   },
   "source": [
    "<a name=\"references\"></a>\n",
    "# üìñ References\n",
    "\n",
    "* Fields of The World: https://fieldsofthe.world/\n",
    "* USDA Cropland Data Layer: https://www.nass.usda.gov/Research_and_Science/Cropland/SARS1a.php\n",
    "* UMD GLAD/Hansen Global Forest Change dataset: https://storage.googleapis.com/earthenginepartners-hansen/GFC-2022-v1.10/download.html\n",
    "* MOSAIKS:\n",
    "  * https://www.nature.com/articles/s41467-021-24638-z\n",
    "  * https://www.mosaiks.org/\n",
    "<!-- * AlphaEarth Embeddings:\n",
    "  * https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/\n",
    "  * https://arxiv.org/abs/2507.22291\n",
    "  * https://medium.com/google-earth/ai-powered-pixels-introducing-googles-satellite-embedding-dataset-31744c1f4650\n",
    "  * https://developers.google.com/earth-engine/tutorials/community/satellite-embedding-01-introduction -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"authors\"></a>\n",
    "# üìß Authors and contact information\n",
    "\n",
    "* Hannah Kerner, Arizona State University (hkerner@asu.edu)\n",
    "* Caleb Robinson, Microsoft AI for Good Lab\n",
    "* Isaac Corley, Wherobots\n",
    "* Gedeon Muhawenayo, Arizona State University\n",
    "* Nathan Jacobs, Washington University in St Louis"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "ftw-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
